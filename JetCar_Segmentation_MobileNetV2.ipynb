{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "JetCar_Segmentation_MobileNetV2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCSP-dbMw88x"
      },
      "source": [
        "# Image segmentation for JetCar project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NEWs8JXRuGex"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",       
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/github.com/StefanW0815/PublicTest/blob/main/JetCar_Segmentation_MobileNetV2.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueWQRNmPSLos"
      },
      "source": [
        "# 1. Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2i0hkSrSLot"
      },
      "source": [
        "This notebook uses Semantic Segmentation to train a U-Net model. Each training and validation data pair consist of\n",
        "\n",
        "    a jpg image with 3 channels (RGB)\n",
        "    a png mask with 1 channel contining the class values for each pixel\n",
        "    \n",
        "In addition there is a set of images in a third set to create predicted masks as feedback for segmentation mask adjustments.\n",
        "\n",
        "The code is based on following examples:\n",
        "* https://www.tensorflow.org/tutorials/images/segmentation\n",
        "* https://yann-leguilly.gitlab.io/post/2019-12-14-tensorflow-tfdata-segmentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53uMkooT79_4"
      },
      "source": [
        "# 2. Preparing the Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0lCFAUR97ZDR"
      },
      "source": [
        "from glob import glob\n",
        "import shutil\n",
        "import argparse\n",
        "import zipfile\n",
        "import hashlib\n",
        "import requests\n",
        "from tqdm import tqdm\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import datetime, os\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from IPython.display import clear_output\n",
        "import tensorflow_addons as tfa\n",
        "from urllib.parse import urlparse\n",
        "import zipfile\n",
        "\n",
        "# For more information about autotune:\n",
        "# https://www.tensorflow.org/guide/data_performance#prefetching\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "print(f\"Tensorflow ver. {tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdd0yn2pfOKT"
      },
      "source": [
        "!pip install git+https://github.com/tensorflow/examples.git\n",
        "!pip install -U tfds-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGKYiwWM7my6"
      },
      "source": [
        "# important for reproducibility\n",
        "# this allows to generate the same random numbers\n",
        "SEED = 42"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s20blc8USLo_"
      },
      "source": [
        "import tensorflow as tf \n",
        "print(tf.test.is_built_with_cuda()) \n",
        "print(tf.config.list_physical_devices('GPU'))\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "    try:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "    except RuntimeError as e:\n",
        "        print(e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzTZK_AnTUUv"
      },
      "source": [
        "### 2.1. Downloading the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18GQXNxaTUUw"
      },
      "source": [
        "The data can be downloaded from a google drive as shown in cells below. Alternatively the path definitions can be changed to get data from a local drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRsZcrr-SLpF"
      },
      "source": [
        "GDRIVE_MOUNT = '/content/gdrive/'\n",
        "GDRIVE_PATH = 'gdrive/My Drive/JetCar/'\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(GDRIVE_MOUNT, force_remount=True)\n",
        "!ls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-c5Q1n1tWICD"
      },
      "source": [
        "\n",
        "!ls gdrive\n",
        "!ls \"gdrive/My Drive\"\n",
        "!ls \"gdrive/My Drive/JetCar/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4miLsVV5SLpK"
      },
      "source": [
        "DOWNLOAD_URLS = [\n",
        "      GDRIVE_PATH+'JetCar_DataSet.zip',\n",
        "      GDRIVE_PATH+'JetCar_Recording.zip']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VW3jgp_1_q-6"
      },
      "source": [
        "#Local drive path definitions if data is available locally\n",
        "DATA_PATH = \"/content/JetCar/\"\n",
        "print(DATA_PATH)\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    os.makedirs(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eJtrVRxeSLpT"
      },
      "source": [
        "DATASET_PATH = DATA_PATH + \"DataSet/\"\n",
        "IMAGE_SUBDIR = \"Img/\"\n",
        "TRAINING_SUBDIR = \"Train/\"\n",
        "VALIDATION_SUBDIR = \"Val/\"\n",
        "RECORDING_PATH = DATA_PATH +\"Recording/\"\n",
        "PREDICTION_PATH = DATA_PATH +\"Prediction/\"\n",
        "MODEL_WEIGHT_FILE_NAME = 'best_model.h5'\n",
        "PREDICTION_ZIP_FILE_NAME = 'Prediction.zip'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yocgxYITGXSv"
      },
      "source": [
        "## 3. Creating our Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KqCjhW6-Rdl"
      },
      "source": [
        "# Image size that we are going to use\n",
        "IMG_SIZE = 224 \n",
        "# Our images are RGB (3 channels)\n",
        "N_CHANNELS = 3\n",
        "# Scene Parsing has 22 classes including 'nothing'=0\n",
        "N_CLASSES = 22"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gU3wT9USLpc"
      },
      "source": [
        "def download(source_url, destination_path=None, overwrite=False):\n",
        "\n",
        "    if destination_path is None:\n",
        "        fname = source_url.split('/')[-1]\n",
        "    else:\n",
        "        destination_path = os.path.expanduser(destination_path)\n",
        "        if os.path.isdir(destination_path):\n",
        "            fname = os.path.join(destination_path, source_url.split('/')[-1])\n",
        "        else:\n",
        "            fname = destination_path\n",
        "\n",
        "    if overwrite or not os.path.exists(fname):\n",
        "        dirname = os.path.dirname(os.path.abspath(os.path.expanduser(fname)))\n",
        "        if not os.path.exists(dirname):\n",
        "            os.makedirs(dirname)\n",
        "\n",
        "        scheme = urlparse(source_url).scheme\n",
        "        if scheme == 'http' or scheme == 'https':\n",
        "            print('Downloading %s from %s...'%(fname, source_url))\n",
        "            r = requests.get(source_url, stream=True)\n",
        "            if r.status_code != 200:\n",
        "                raise RuntimeError(\"Failed downloading url %s\"%url)\n",
        "            total_length = r.headers.get('content-length')\n",
        "           \n",
        "            with open(fname, 'wb') as f:\n",
        "                if total_length is None: # no content length header\n",
        "                    for chunk in r.iter_content(chunk_size=1024):\n",
        "                        if chunk: # filter out keep-alive new chunks\n",
        "                            f.write(chunk)\n",
        "                else:\n",
        "                    total_length = int(total_length)\n",
        "                    for chunk in tqdm(r.iter_content(chunk_size=1024),\n",
        "                                      total=int(total_length / 1024. + 0.5),\n",
        "                                      unit='KB', unit_scale=False, dynamic_ncols=True):\n",
        "                        f.write(chunk)\n",
        "\n",
        "        else:\n",
        "            print('Copying %s from %s...'%(os.path.normpath(fname), os.path.normpath(source_url)))\n",
        "            tf.io.gfile.copy(os.path.normpath(source_url), os.path.normpath(fname), overwrite)\n",
        "\n",
        "    return fname\n",
        "\n",
        "def download_dataset(source_urls, destination_path, overwrite=False):\n",
        "    if not os.path.exists(destination_path):\n",
        "        os.mkdir(destination_path)\n",
        "    download_dir = os.path.join(destination_path, 'downloads')\n",
        "    if not os.path.exists(download_dir):\n",
        "        os.mkdir(download_dir)\n",
        "    for url in source_urls:\n",
        "        filename = download(source_url=url, destination_path=download_dir, overwrite=overwrite)\n",
        "        # extract\n",
        "        with zipfile.ZipFile(filename,\"r\") as zip_ref:\n",
        "            zip_ref.extractall(path=destination_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFy2ARWoSLpg"
      },
      "source": [
        "download_dataset(DOWNLOAD_URLS, DATA_PATH, overwrite=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WrK2kdrTSLpk"
      },
      "source": [
        "if os.path.exists(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME):\n",
        "    print('Copying %s from %s...'%(MODEL_WEIGHT_FILE_NAME, GDRIVE_PATH))\n",
        "    tf.io.gfile.copy(os.path.normpath(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME), \n",
        "                 os.path.normpath(DATA_PATH + MODEL_WEIGHT_FILE_NAME), True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qen8EpZ9TUVx"
      },
      "source": [
        "### 3.1. Creating a source dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpn9QVL2SLpq"
      },
      "source": [
        "In addition to training and validation data sets, there is another image only dataset to create prediction masks from. These masks are used as feedback for adjusting the classification if needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7znO6EafQs8S"
      },
      "source": [
        "TRAINSET_SIZE = len(glob(DATASET_PATH + IMAGE_SUBDIR + TRAINING_SUBDIR + \"*.jpg\"))\n",
        "print(f\"The Training Dataset contains {TRAINSET_SIZE} images.\")\n",
        "\n",
        "VALSET_SIZE = len(glob(DATASET_PATH + IMAGE_SUBDIR + VALIDATION_SUBDIR + \"*.jpg\"))\n",
        "print(f\"The Validation Dataset contains {VALSET_SIZE} images.\")\n",
        "\n",
        "PREDSET_SIZE = len(glob(RECORDING_PATH + \"*.jpg\"))\n",
        "print(f\"The Prediction Dataset contains {PREDSET_SIZE} images.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3IeIvttGer6"
      },
      "source": [
        "For each images of our dataset, we will apply some operations wrapped into a function. Then we will map the whole dataset with this function.   \n",
        "\n",
        "So let's write this function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "REeU48WsGub0"
      },
      "source": [
        " def parse_dataset_image(img_path: str) -> dict:\n",
        "    image = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "    mask_path = tf.strings.regex_replace(img_path, \"Img\", \"Mask\")\n",
        "    mask_path = tf.strings.regex_replace(mask_path, \"jpg\", \"png\")\n",
        "    mask = tf.io.read_file(mask_path)\n",
        "    # The masks contain a class index for each pixels\n",
        "    mask = tf.image.decode_png(mask, channels=1)\n",
        "\n",
        "    return {'image': image, 'segmentation_mask': mask}\n",
        "\n",
        "\n",
        "def parse_recording_image(img_path: str) -> dict:\n",
        "    image = tf.io.read_file(img_path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "\n",
        "    return {'image': image, 'img_path': img_path}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Vpo8nhYGwy6"
      },
      "source": [
        "train_dataset = tf.data.Dataset.list_files(DATASET_PATH + IMAGE_SUBDIR + TRAINING_SUBDIR + \"*.jpg\", seed=SEED)\n",
        "train_dataset = train_dataset.map(parse_dataset_image)\n",
        "\n",
        "val_dataset = tf.data.Dataset.list_files(DATASET_PATH + IMAGE_SUBDIR + VALIDATION_SUBDIR + \"*.jpg\", seed=SEED)\n",
        "val_dataset = val_dataset.map(parse_dataset_image)\n",
        "\n",
        "pred_dataset = tf.data.Dataset.list_files(RECORDING_PATH + \"*.jpg\", seed=SEED)\n",
        "pred_dataset = pred_dataset.map(parse_recording_image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9fjcxBaTUWT"
      },
      "source": [
        "### 3.2. Applying some transformations to our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0Pa1mgPSSOL"
      },
      "source": [
        "\"\"\"Resize and normalize\"\"\"\n",
        "@tf.function\n",
        "def load_image_train_val(datapoint: dict) -> tuple:\n",
        "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
        "    input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    return input_image, input_mask\n",
        "\n",
        "@tf.function\n",
        "def load_image_pred(datapoint: dict) -> tuple:\n",
        "    input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
        "    input_image = tf.cast(input_image, tf.float32) / 255.0\n",
        "    return input_image, datapoint['img_path']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6n6lynNAZ2z"
      },
      "source": [
        "Manipulating datasets in tensorflow can be complicated. You can read the official documentation to understand how they are working: https://www.tensorflow.org/guide/data#training_workflows"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0BpQ8uwHXwb"
      },
      "source": [
        "BATCH_SIZE = 10\n",
        "BUFFER_SIZE = 1000\n",
        "TRAIN_LENGTH = TRAINSET_SIZE\n",
        "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE\n",
        "\n",
        "dataset = {\"train\": train_dataset, \"val\": val_dataset, \"pred\": pred_dataset}\n",
        "\n",
        "# -- Train Dataset --#\n",
        "dataset['train'] = dataset['train'].map(load_image_train_val, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
        "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
        "dataset['train'] = dataset['train'].repeat()\n",
        "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
        "dataset['train'] = dataset['train'].prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "#-- Validation Dataset --#\n",
        "dataset['val'] = dataset['val'].map(load_image_train_val)\n",
        "dataset['val'] = dataset['val'].repeat()\n",
        "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
        "dataset['val'] = dataset['val'].prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "#-- Prediction Dataset --#\n",
        "dataset['pred'] = dataset['pred'].map(load_image_pred)\n",
        "dataset['pred'] = dataset['pred'].batch(BATCH_SIZE)\n",
        "dataset['pred'] = dataset['pred'].prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "print(dataset['train'])\n",
        "print(dataset['val'])\n",
        "print(dataset['pred'])\n",
        "\n",
        "print(\"TRAIN_LENGTH = %d   STEPS_PER_EPOCH = %d \" %(TRAIN_LENGTH,STEPS_PER_EPOCH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39fYScNz9lmo"
      },
      "source": [
        "train = dataset['train']\n",
        "test = dataset['val']\n",
        "pred = dataset['pred']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xa3gMAE_9qNa"
      },
      "source": [
        "Let's take a look at an image example and it's correponding mask from the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3N2RPAAW9q4W"
      },
      "source": [
        "def display(display_list):\n",
        "  plt.figure(figsize=(15, 15))\n",
        "\n",
        "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
        "\n",
        "  for i in range(len(display_list)):\n",
        "    plt.subplot(1, len(display_list), i+1)\n",
        "    plt.title(title[i])\n",
        "    plt.imshow(tf.keras.preprocessing.image.array_to_img(display_list[i]))\n",
        "    plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6u_Rblkteqb"
      },
      "source": [
        "for image, mask in train.take(1):\n",
        "  sample_image, sample_mask = image[0], mask[0]\n",
        "display([sample_image, sample_mask])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FAOe93FRMk3w"
      },
      "source": [
        "## Define the model\n",
        "The model being used here is a modified U-Net. A U-Net consists of an encoder (downsampler) and decoder (upsampler). In-order to learn robust features, and reduce the number of trainable parameters, a pretrained model can be used as the encoder. Thus, the encoder for this task will be a pretrained MobileNetV2 model, whose intermediate outputs will be used, and the decoder will be the upsample block already implemented in TensorFlow Examples in the [Pix2pix tutorial](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py). \n",
        "\n",
        "The reason to output N_CLASSES channels is because there are three possible labels for each pixel. Think of this as multi-classification where each pixel is being classified into N_CLASSES classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6iB4iMvMkX9"
      },
      "source": [
        "OUTPUT_CHANNELS = N_CLASSES\n",
        "IMG_WIDTH=IMG_SIZE\n",
        "IMG_HEIGHT=IMG_SIZE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4mQle3lthit"
      },
      "source": [
        "As mentioned, the encoder will be a pretrained MobileNetV2 model which is prepared and ready to use in [tf.keras.applications](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/applications). The encoder consists of specific outputs from intermediate layers in the model. Note that the encoder will not be trained during the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liCeLH0ctjq7"
      },
      "source": [
        "base_model = tf.keras.applications.MobileNetV2(input_shape=[IMG_WIDTH, IMG_HEIGHT, 3], include_top=False)\n",
        "\n",
        "# Use the activations of these layers\n",
        "layer_names = [\n",
        "    'block_1_expand_relu',   # 64x64\n",
        "    'block_3_expand_relu',   # 32x32\n",
        "    'block_6_expand_relu',   # 16x16\n",
        "    'block_13_expand_relu',  # 8x8\n",
        "    'block_16_project',      # 4x4\n",
        "]\n",
        "layers = [base_model.get_layer(name).output for name in layer_names]\n",
        "\n",
        "# Create the feature extraction model\n",
        "down_stack = tf.keras.Model(inputs=base_model.input, outputs=layers)\n",
        "\n",
        "down_stack.trainable = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPw8Lzra5_T9"
      },
      "source": [
        "The decoder/upsampler is simply a series of upsample blocks implemented in TensorFlow examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ey7DRJ5vSLqR"
      },
      "source": [
        "from tensorflow_examples.models.pix2pix import pix2pix\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ZbfywEbZpJ"
      },
      "source": [
        "up_stack = [\n",
        "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
        "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
        "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
        "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45HByxpVtrPF"
      },
      "source": [
        "def unet_model(output_channels):\n",
        "  inputs = tf.keras.layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, 3])\n",
        "  x = inputs\n",
        "\n",
        "  # Downsampling through the model\n",
        "  skips = down_stack(x)\n",
        "  x = skips[-1]\n",
        "  skips = reversed(skips[:-1])\n",
        "\n",
        "  # Upsampling and establishing the skip connections\n",
        "  for up, skip in zip(up_stack, skips):\n",
        "    x = up(x)\n",
        "    concat = tf.keras.layers.Concatenate()\n",
        "    x = concat([x, skip])\n",
        "\n",
        "  # This is the last layer of the model\n",
        "  last = tf.keras.layers.Conv2DTranspose(\n",
        "      output_channels, 3, strides=2,\n",
        "      padding='same')  #64x64 -> 128x128\n",
        "\n",
        "  x = last(x)\n",
        "\n",
        "  return tf.keras.Model(inputs=inputs, outputs=x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0DGH_4T0VYn"
      },
      "source": [
        "## Train the model\n",
        "Now, all that is left to do is to compile and train the model. The loss being used here is `losses.SparseCategoricalCrossentropy(from_logits=True)`. The reason to use this loss function is because the network is trying to assign each pixel a label, just like multi-class prediction. In the true segmentation mask, each pixel has one of the values between 0 and N_CLASSES-1. The network here is outputting N_CLASSES channels. Essentially, each channel is trying to learn to predict a class, and `losses.SparseCategoricalCrossentropy(from_logits=True)` is the recommended loss for \n",
        "such a scenario. Using the output of the network, the label assigned to the pixel is the channel with the highest value. This is what the create_mask function is doing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6he36HK5uKAc"
      },
      "source": [
        "model = unet_model(OUTPUT_CHANNELS)\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=1e-5),\n",
        "              #optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud0ZtrJCSLqe"
      },
      "source": [
        "if os.path.exists(DATA_PATH + MODEL_WEIGHT_FILE_NAME): \n",
        "    model.load_weights(DATA_PATH + MODEL_WEIGHT_FILE_NAME)\n",
        "    print('Last model weights loaded!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVMzbIZLcyEF"
      },
      "source": [
        "Have a quick look at the resulting model architecture:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw82qF1Gcovr"
      },
      "source": [
        "#tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc3MiEO2twLS"
      },
      "source": [
        "Let's try out the model to see what it predicts before training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwvIKLZPtxV_"
      },
      "source": [
        "def create_mask(pred_mask):\n",
        "  pred_mask = tf.argmax(pred_mask, axis=-1)\n",
        "  pred_mask = pred_mask[..., tf.newaxis]\n",
        "  return pred_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLNsrynNtx4d"
      },
      "source": [
        "def show_predictions(dataset=None, num=1):\n",
        "  if dataset:\n",
        "    for image, mask in dataset.take(num):\n",
        "      pred_mask = model.predict(image)\n",
        "      pred_mask = create_mask(pred_mask);\n",
        "      display([image[0], mask[0], pred_mask[0] ])\n",
        "  else:\n",
        "    pred_mask = create_mask(model.predict(sample_image[tf.newaxis, ...]))[0]\n",
        "    display([sample_image, sample_mask, pred_mask ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_1CC0T4dho3"
      },
      "source": [
        "show_predictions()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22AyVYWQdkgk"
      },
      "source": [
        "Let's observe how the model improves while it is training. To accomplish this task, a callback function is defined below. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHrHsqijdmL6"
      },
      "source": [
        "class DisplayCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    #import pdb; pdb.set_trace()\n",
        "    clear_output(wait=True)\n",
        "    show_predictions()\n",
        "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StKDH_B9t4SD",
        "scrolled": false
      },
      "source": [
        "EPOCHS = 100\n",
        "STEPS_PER_EPOCH = TRAINSET_SIZE // BATCH_SIZE\n",
        "VALIDATION_STEPS = VALSET_SIZE // BATCH_SIZE\n",
        "PREDICTION_STEPS = PREDSET_SIZE // BATCH_SIZE\n",
        "\n",
        "print(\"STEPS_PER_EPOCH = %d   VALIDATION_STEPS = %d   PREDICTION_STEPS = %d \" \n",
        "      %(STEPS_PER_EPOCH, VALIDATION_STEPS, PREDICTION_STEPS))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfmvItvJSLq4"
      },
      "source": [
        "log_name = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "logdir = os.path.join(\"logs\", log_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0BX0TMHSLq7"
      },
      "source": [
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, histogram_freq=1)\n",
        "\n",
        "callbacks = [\n",
        "    # to show samples after each epoch\n",
        "    DisplayCallback(),\n",
        "    # to collect some useful metrics and visualize them in tensorboard\n",
        "    tensorboard_callback,\n",
        "    # if no accuracy improvements we can stop the training directly\n",
        "    tf.keras.callbacks.EarlyStopping(patience=10, verbose=1),\n",
        "    # to save checkpoints\n",
        "    tf.keras.callbacks.ModelCheckpoint(DATA_PATH + MODEL_WEIGHT_FILE_NAME, \n",
        "                                       verbose=1, save_best_only=True, save_weights_only=True)\n",
        "]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r31clOSQSLq9"
      },
      "source": [
        "# Load the TensorBoard notebook extension.\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LaPKA60VTUX9"
      },
      "source": [
        "model_history = model.fit(dataset['train'], epochs=EPOCHS,\n",
        "                    steps_per_epoch=STEPS_PER_EPOCH,\n",
        "                    validation_steps=VALIDATION_STEPS,\n",
        "                    validation_data=dataset['val'],\n",
        "                    callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwKBnYaESLrA"
      },
      "source": [
        "if os.path.exists(DATA_PATH + MODEL_WEIGHT_FILE_NAME): \n",
        "    print('Copying %s to %s...'%(MODEL_WEIGHT_FILE_NAME, GDRIVE_PATH))\n",
        "    tf.io.gfile.copy(os.path.normpath(DATA_PATH + MODEL_WEIGHT_FILE_NAME), \n",
        "                     os.path.normpath(GDRIVE_PATH + MODEL_WEIGHT_FILE_NAME), True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQ8aGwUMSLrC"
      },
      "source": [
        "import zipfile\n",
        "if os.path.exists(logdir): \n",
        "    print(\"Compressing logs\")\n",
        "    zipFileName = logdir+\".zip\"\n",
        "    zipObj = zipfile.ZipFile(zipFileName, 'w')\n",
        "    zipObj.write(logdir)\n",
        "    zipObj.close()\n",
        "    print('Copying %s to %s...'%(zipFileName, GDRIVE_PATH))\n",
        "    tf.io.gfile.copy(os.path.normpath(zipFileName), \n",
        "                     os.path.normpath(GDRIVE_PATH + log_name+\".zip\"), True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unP3cnxo_N72"
      },
      "source": [
        "## Make predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BVXldSo-0mW"
      },
      "source": [
        "Using the training results prediction masks will be generated for a separete set of images than for training and validation. This can be the original set of images before augmentation. The prediction can be used as feedback to fine tune the segmentation mask generation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCTQbSEgSLrF"
      },
      "source": [
        "if not os.path.exists(PREDICTION_PATH):\n",
        "    os.makedirs(PREDICTION_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Yw12Uf0SLrH"
      },
      "source": [
        "if os.path.exists(PREDICTION_PATH + PREDICTION_ZIP_FILE_NAME):\n",
        "    tf.io.gfile.remove(PREDICTION_PATH + PREDICTION_ZIP_FILE_NAME)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ux9za2YSLrK"
      },
      "source": [
        "import zipfile\n",
        "zipObj = zipfile.ZipFile(PREDICTION_PATH+PREDICTION_ZIP_FILE_NAME, 'w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZ4syXq2SLrM"
      },
      "source": [
        "n=0\n",
        "for step, (image_batch, filename_batch) in enumerate(pred):\n",
        "    pred_batch = create_mask(model.predict(image_batch))\n",
        "    for i in range(image_batch.shape[0]):\n",
        "        base_filename = filename_batch[i].numpy().decode('ascii')\n",
        "        base_filename = os.path.basename(base_filename)\n",
        "        base_filename = os.path.splitext(base_filename)[0]\n",
        "        pred_filename = base_filename.replace(\"Img\",\"Pred\")+\".png\"\n",
        "        n = n+1\n",
        "        print(f\"Saving file #{n}: \" + pred_filename)\n",
        "        tf.keras.preprocessing.image.save_img(PREDICTION_PATH + pred_filename, pred_batch[i], scale=False)\n",
        "        zipObj.write(PREDICTION_PATH + pred_filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1VHZGYgfSLrO"
      },
      "source": [
        "zipObj.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVIPzvvrSLrQ"
      },
      "source": [
        "print('Copying %s to %s...'%(PREDICTION_ZIP_FILE_NAME, GDRIVE_PATH))\n",
        "tf.io.gfile.copy(os.path.normpath(PREDICTION_PATH+PREDICTION_ZIP_FILE_NAME), \n",
        "                 os.path.normpath(GDRIVE_PATH+PREDICTION_ZIP_FILE_NAME), True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5roLmFjzSLrS"
      },
      "source": [
        "print(\"All Done!\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
